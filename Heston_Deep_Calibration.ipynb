{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Aim of this notebook is to calibrate the Heston model using deep learning technique introduced by *Horvath et al.* in *Deep Learning Volatility*\n",
    "\n",
    "\n",
    "The notebook will replicate the model given in the paper and use SPX option prices from Yahoo Finance for testing. Deep learning training data will be generated synthetically.\n",
    "\n",
    "\n",
    " References: https://arxiv.org/abs/1901.09647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will run/train on: cuda\n"
     ]
    }
   ],
   "source": [
    "# import libaries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import mse_loss, l1_loss\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time \n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Model will run/train on:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeds to allow reproducibility \n",
    "\n",
    "seed_val = 0\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "gen1 = torch.Generator().manual_seed(0)\n",
    "\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model paramters\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 200\n",
    "\n",
    "run_training = True\n",
    "\n",
    "generate_training_data = False\n",
    " \n",
    "strikes = np.arange(5800,6900, step = 100).tolist()\n",
    "\n",
    "maturities = np.arange(0.048,0.11, step = 1/252).round(3).tolist()\n",
    "\n",
    "maturities_year = np.arange(0.0,1, step = 1/252).round(3).tolist()\n",
    "\n",
    "maturities_idx = [12,14,16,18,20,22,24,26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NelsonSiegelSvenssonCurve(beta0=0.04563032361939878, beta1=0.0036849763147745952, beta2=-0.036081271617350925, beta3=0.01048653576721284, tau1=2.0, tau2=5.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nelson_siegel_svensson import NelsonSiegelSvenssonCurve\n",
    "from nelson_siegel_svensson.calibrate import calibrate_nss_ols\n",
    "\n",
    "yield_maturities = np.array([1/12, 2/12, 3/12,4/12, 6/12, 1, 2, 3, 5, 7, 10, 20, 30])\n",
    "yields = np.array([4.92,4.82,4.73,4.66,4.47,4.24,4.02,3.95,3.98,4.07,4.19,4.54,4.49]).astype(float)/100\n",
    "\n",
    "#NSS model calibrate\n",
    "curve_fit, status = calibrate_nss_ols(yield_maturities,yields)\n",
    "\n",
    "#Can use the fitted curve to get rate at any tenor point\n",
    "curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_heston_paths(S0, V0, mu, kappa, theta, sigma, rho, T, dt, n_paths, rates_curve = curve_fit):\n",
    "    \"\"\"\n",
    "    Generate synthetic paths for the Heston model.\n",
    "    \n",
    "    Parameters:\n",
    "    - S0: Initial asset price\n",
    "    - V0: Initial variance\n",
    "    - mu: Drift term\n",
    "    - kappa: Mean reversion rate of the variance\n",
    "    - theta: Long-term variance\n",
    "    - sigma: Volatility of volatility\n",
    "    - rho: Correlation between the asset and variance processes\n",
    "    - T: Time horizon (in years)\n",
    "    - dt: Time step size\n",
    "    - n_paths: Number of paths to simulate\n",
    "    \n",
    "    Returns:\n",
    "    - S_paths: Simulated asset price paths (shape: n_paths x n_time_steps)\n",
    "    - V_paths: Simulated variance paths (shape: n_paths x n_time_steps)\n",
    "    \"\"\"\n",
    "    n_steps = int(T / dt)\n",
    "    S_paths = np.zeros((n_paths, n_steps + 1))\n",
    "    V_paths = np.zeros((n_paths, n_steps + 1))\n",
    "    \n",
    "    # Initial conditions\n",
    "    S_paths[:, 0] = S0\n",
    "    V_paths[:, 0] = V0\n",
    "    \n",
    "    # Generate correlated random numbers for the two Wiener processes\n",
    "    Z1 = np.random.normal(size=(n_paths, n_steps))\n",
    "    Z2 = np.random.normal(size=(n_paths, n_steps))\n",
    "    W1 = Z1\n",
    "    W2 = rho * Z1 + np.sqrt(1 - rho**2) * Z2\n",
    "    \n",
    "    # Time stepping through the paths\n",
    "    for t in range(1, n_steps + 1):\n",
    "        V_paths[:, t] = V_paths[:, t-1] + kappa * (theta - V_paths[:, t-1]) * dt + \\\n",
    "                        sigma * np.sqrt(np.maximum(V_paths[:, t-1], 0)) * np.sqrt(dt) * W2[:, t-1]\n",
    "        \n",
    "        # Ensure variance remains non-negative\n",
    "        V_paths[:, t] = np.maximum(V_paths[:, t], 0)\n",
    "        \n",
    "        S_paths[:, t] = S_paths[:, t-1] * np.exp((mu - 0.5 * V_paths[:, t-1]) * dt + \\\n",
    "                         np.sqrt(V_paths[:, t-1] * dt) * W1[:, t-1])\n",
    "\n",
    "        \n",
    "    return S_paths, V_paths\n",
    "\n",
    "# Example usage\n",
    "S0 = 5500  # Initial asset price\n",
    "V0 = 0.04  # Initial variance (volatility squared)\n",
    "mu = 0.05  # Drift\n",
    "kappa = 2.0  # Mean reversion rate\n",
    "theta = 0.04  # Long-term variance\n",
    "sigma = 0.5  # Volatility of volatility\n",
    "rho = -0.7  # Correlation between the two Wiener processes\n",
    "T = 1.0  # 1 year\n",
    "dt = 1/252  # Daily time steps\n",
    "n_paths = 1000  # Number of simulated paths\n",
    "\n",
    "S_paths, V_paths = generate_heston_paths(S0, V0, mu, kappa, theta, sigma, rho, T, dt, n_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_values = np.arange(0.01,0.11, step = 0.01).round(2).tolist()\n",
    "kappa_values = np.arange(1.25,3.75, step = 0.25).round(2).tolist()\n",
    "theta_values = np.arange(0.02,0.12, step = 0.01).round(2).tolist()\n",
    "sigma_values = np.arange(0.1,1.1, step = 0.1).round(2).tolist()\n",
    "rho_values = np.arange(-0.1,-1.1, step = -0.1).round(2).tolist()\n",
    "\n",
    "total_combinations = len(mu_values) * len(kappa_values) * len(theta_values) * len(sigma_values) * len(rho_values)\n",
    "\n",
    "training_option_prices = []\n",
    "training_option_params = []\n",
    "\n",
    "\n",
    "if generate_training_data:\n",
    "\n",
    "# Iterate over all combinations of parameter sets with a progress bar\n",
    "    for mu, kappa, theta, sigma, rho in tqdm(itertools.product(mu_values, kappa_values, theta_values, sigma_values, rho_values), total=total_combinations):\n",
    "        S_paths, V_paths = generate_heston_paths(S0, V0, mu, kappa, theta, sigma, rho, T, dt, n_paths)\n",
    "        #training_paths.append([mu, kappa, theta, sigma, rho, S_paths[], V_paths])\n",
    "\n",
    "        temp = np.zeros((n_paths, len(maturities_idx) +1))\n",
    "        option_prices = np.zeros((len(strikes), len(maturities_idx)))\n",
    "        for k_idx, k in enumerate(strikes):\n",
    "            for i, t_idx in enumerate(maturities_idx):\n",
    "                t = maturities_year[t_idx]\n",
    "                payoffs = np.maximum(S_paths[:, t_idx] - k, 0)  # Payoff for call option\n",
    "                temp[:,i ] = payoffs * np.exp(-curve_fit(T- t * dt)* (T - t * dt))\n",
    "                option_prices[k_idx, :] = temp[:,i].mean(axis = 0)\n",
    "        #Store or use the generated paths (S_paths) for training your model\n",
    "        training_option_prices.append(option_prices)\n",
    "        training_option_params.append([mu, kappa, theta, sigma, rho])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save copy to save time \n",
    "if generate_training_data:\n",
    "    with open('../equity/training_option_prices.pkl', 'wb') as output:\n",
    "        pickle.dump(training_option_prices, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load output to save time\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mgenerate_training_data\u001b[49m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../equity/training_option_prices.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      4\u001b[0m         training_option_prices \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generate_training_data' is not defined"
     ]
    }
   ],
   "source": [
    "# load output to save time\n",
    "if not generate_training_data:\n",
    "    with open('../equity/training_option_prices.pkl', 'rb') as file:\n",
    "        training_option_prices = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_prices = []\n",
    "op_params = []\n",
    "\n",
    "for i in training_option_prices:\n",
    "    op_prices.append(i[5])\n",
    "    op_params.append(i[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MITESH\\AppData\\Local\\Temp\\ipykernel_36984\\2379886255.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  op_prices = torch.Tensor(op_prices)\n"
     ]
    }
   ],
   "source": [
    "op_prices = torch.Tensor(op_prices)\n",
    "\n",
    "op_params = torch.Tensor(op_params)\n",
    "\n",
    "train_size = int(0.8 * len(op_params))  # 80% for training\n",
    "val_size = len(op_params) - train_size\n",
    "\n",
    "training_data = TensorDataset(op_params, op_prices)\n",
    "\n",
    "train_dataset, val_dataset = random_split(training_data, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The deep learning model is a made up of four fully-connected hidden layers each consisting of 30 nodes. The input layer (our Heston model paramters are the inputs) is of size $n$ and output layer is given in the paper as $11$ strikes $\\times 8 $ maturities. The paper states the output layer can be edited by the end user. For now we will keep it the same, if it is changed this will be indicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCalibration(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.fc1 = nn.Sequential(nn.Linear(self.input_size,30), nn.ELU())\n",
    "        self.fc2 = nn.Sequential(nn.Linear(30,30), nn.ELU())\n",
    "        self.fc3 = nn.Sequential(nn.Linear(30,30), nn.ELU())\n",
    "        self.fc4 = nn.Linear(30,self.output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #assert x.size(-1) == self.input_size\n",
    "\n",
    "        h1 = self.fc1(x)\n",
    "        h2 =self.fc2(h1)\n",
    "        h3 =self.fc3(h2)\n",
    "        h4 =self.fc4(h3)\n",
    "\n",
    "        return h4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise models and load to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_cal = DeepCalibration(input_size= 5, output_size= 88)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    deep_cal.to(\"cuda\")  \n",
    "\n",
    "#summary(deep_cal, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataloaders to allow efficient data batching for training/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_loader  = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "def train_model(dl_model,epochs, training_dl, validation_dl, \n",
    "                autograd, dlmodel_loc, trainloss_loc, valloss_loc, trainset_size,valset_size, \n",
    "                learning_rate = 0.0001, mse_multp = 1.0, mae_multp = 1.0, return_model = False):\n",
    "    # define loss and parameters\n",
    "    \n",
    "    dl_model.train()\n",
    "\n",
    "    optimiser = autograd(itertools.chain(dl_model.parameters()), lr=learning_rate)\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "    val_loss_check = 1e10\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    print('====Training started====')\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        total_mseloss, total_maeloss = 0.0, 0.0\n",
    "        total_val_loss, total_train_loss = 0.0, 0.0\n",
    "        val_loss_mse, val_loss_mae = 0.0, 0.0\n",
    "\n",
    "        for batch_idx, (data, label) in enumerate(training_dl):\n",
    "            # prepare input data\n",
    "            img = data.to(\"cuda\")\n",
    "            lab = label.to(\"cuda\")\n",
    "            \n",
    "            model_output = dl_model(img)\n",
    "            \n",
    "            loss_mse = mse_loss(model_output.reshape(lab.shape), lab)\n",
    "            \n",
    "            loss_mae = l1_loss(model_output.reshape(lab.shape), lab)\n",
    "        \n",
    "            loss = (mse_multp * loss_mse) + (mae_multp * loss_mae)\n",
    "            \n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            # Track this epoch's training loss\n",
    "            total_mseloss += loss_mse.item() * data.shape[0] / trainset_size\n",
    "            total_maeloss += loss_mae.item() * data.shape[0] / trainset_size\n",
    "            total_train_loss += (loss_mse.item() + loss_mae.item()) * data.shape[0] / trainset_size\n",
    "\n",
    "        train_loss.append([total_mseloss, total_maeloss , total_train_loss])\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        dl_model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx_val, (data_val, label_val) in enumerate(validation_dl):\n",
    "                \n",
    "                img_val = data_val.to(\"cuda\")\n",
    "                lab_val = label_val.to(\"cuda\")\n",
    "                \n",
    "                output_val = dl_model(img_val).reshape(lab_val.shape)\n",
    "                \n",
    "                # Track this epoch's validation loss\n",
    "                val_loss_mse += mse_loss(output_val, lab_val).item() * data_val.shape[0] / valset_size\n",
    "                val_loss_mae += l1_loss(output_val, lab_val).item() * data_val.shape[0] / valset_size\n",
    "                total_val_loss += (\n",
    "                                    (mse_loss(output_val, lab_val).item() * val_loss_mse) +  \n",
    "                                    (torch.sum(l1_loss(output_val, lab_val)).item()  * mae_multp)\n",
    "                                  ) * data_val.shape[0] / valset_size\n",
    "\n",
    "            val_loss.append([val_loss_mse, total_val_loss])\n",
    "\n",
    "        dl_model.train()\n",
    "\n",
    "        t2 = time.time()\n",
    "        #check val error and save model\n",
    "        if epoch > 0:\n",
    "            if total_val_loss < val_loss_check:\n",
    "                torch.save(dl_model, '../' + dlmodel_loc +'intertrain/dlmodel_'+ '_'+ str(epoch) +'_optimal_' + time.strftime(\"%Y%m%d\"))\n",
    "\n",
    "                if os.path.exists('../' + dlmodel_loc +'intertrain/intertrain_model_log.txt') == False:\n",
    "                    open('../' + dlmodel_loc +'intertrain/intertrain_model_log.txt', \"w\").close\n",
    "                    \n",
    "                with open('../' + dlmodel_loc +'intertrain/intertrain_model_log.txt', \"a\") as text_file:\n",
    "                    text_file.write('====> Epoch: {}, Train time: {:.4f}, Val time: {:.4f} \\n'.format(epoch, t1-t0, t2-t1))\n",
    "                    text_file.write('====> Epoch: {} Total loss: {:.4f} MSE Loss: {:.4f} MAE Loss: {:.4f} Val loss: {:.4f} \\n'.format(epoch, total_train_loss, total_mseloss, total_maeloss,total_val_loss))\n",
    "                    text_file.write('../' + dlmodel_loc +'intertrain/dlmodel_' + '_' + str(epoch) +'_optimal_' + time.strftime(\"%Y%m%d\") + '\\n')\n",
    "                    text_file.write('\\n')\n",
    "                \n",
    "                val_loss_check = total_val_loss\n",
    "\n",
    "        if epoch%10==0:  \n",
    "            print('====> Epoch: {}, Train time: {:.4f}, Val time: {:.4f}'.format(epoch, t1-t0, t2-t1))\n",
    "            print('====> Epoch: {} Total loss: {:.4f} MSE Loss: {:.4f} MAE Loss: {:.4f} Val loss: {:.4f}'.format(epoch, total_train_loss, total_mseloss, total_maeloss,total_val_loss))\n",
    "    \n",
    "    print('====Training complete====')\n",
    "\n",
    "    torch.save(dl_model, '../' + dlmodel_loc + '_'+ str(epochs) +'_' + time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "    train_loss_np = np.array(train_loss)\n",
    "    val_loss_np = np.array(val_loss)\n",
    "\n",
    "    with open('../' + trainloss_loc + '_'+ str(epochs) +'_' + time.strftime(\"%Y%m%d-%H%M%S\"), 'wb') as output:\n",
    "        pickle.dump(train_loss_np, output)\n",
    "\n",
    "    with open('../' + valloss_loc + '_' + str(epochs) + '_' + time.strftime(\"%Y%m%d-%H%M%S\"), 'wb') as output:\n",
    "        pickle.dump(val_loss_np, output)\n",
    "\n",
    "    if return_model:\n",
    "        return dl_model, train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Training started====\n",
      "====> Epoch: 0, Train time: 3.3349, Val time: 0.3626\n",
      "====> Epoch: 0 Total loss: 6.8272 MSE Loss: 5.6200 MAE Loss: 1.2072 Val loss: 3.5525\n",
      "====> Epoch: 10, Train time: 41.2923, Val time: 0.4178\n",
      "====> Epoch: 10 Total loss: 3.6000 MSE Loss: 2.6789 MAE Loss: 0.9211 Val loss: 3.6317\n",
      "====> Epoch: 20, Train time: 80.8503, Val time: 0.4009\n",
      "====> Epoch: 20 Total loss: 3.5646 MSE Loss: 2.6516 MAE Loss: 0.9130 Val loss: 3.5015\n",
      "====> Epoch: 30, Train time: 120.6137, Val time: 0.4276\n",
      "====> Epoch: 30 Total loss: 3.5451 MSE Loss: 2.6347 MAE Loss: 0.9104 Val loss: 4.1065\n",
      "====> Epoch: 40, Train time: 163.3823, Val time: 0.4458\n",
      "====> Epoch: 40 Total loss: 3.5518 MSE Loss: 2.6408 MAE Loss: 0.9111 Val loss: 3.1570\n",
      "====> Epoch: 50, Train time: 207.3175, Val time: 0.4535\n",
      "====> Epoch: 50 Total loss: 3.5478 MSE Loss: 2.6374 MAE Loss: 0.9104 Val loss: 3.2845\n",
      "====> Epoch: 60, Train time: 246.8496, Val time: 0.4168\n",
      "====> Epoch: 60 Total loss: 3.5390 MSE Loss: 2.6300 MAE Loss: 0.9091 Val loss: 6.3845\n",
      "====> Epoch: 70, Train time: 285.3820, Val time: 0.4446\n",
      "====> Epoch: 70 Total loss: 3.5542 MSE Loss: 2.6437 MAE Loss: 0.9105 Val loss: 3.3552\n",
      "====> Epoch: 80, Train time: 324.6129, Val time: 0.4092\n",
      "====> Epoch: 80 Total loss: 3.5504 MSE Loss: 2.6391 MAE Loss: 0.9113 Val loss: 4.2794\n",
      "====> Epoch: 90, Train time: 366.4335, Val time: 0.5042\n",
      "====> Epoch: 90 Total loss: 3.5525 MSE Loss: 2.6417 MAE Loss: 0.9108 Val loss: 3.4240\n",
      "====> Epoch: 100, Train time: 406.4371, Val time: 0.4285\n",
      "====> Epoch: 100 Total loss: 3.5399 MSE Loss: 2.6314 MAE Loss: 0.9085 Val loss: 3.5026\n",
      "====> Epoch: 110, Train time: 446.8009, Val time: 0.4532\n",
      "====> Epoch: 110 Total loss: 3.5470 MSE Loss: 2.6367 MAE Loss: 0.9104 Val loss: 3.1070\n",
      "====> Epoch: 120, Train time: 488.1760, Val time: 0.4667\n",
      "====> Epoch: 120 Total loss: 3.5475 MSE Loss: 2.6372 MAE Loss: 0.9103 Val loss: 3.2232\n",
      "====> Epoch: 130, Train time: 534.6552, Val time: 0.4452\n",
      "====> Epoch: 130 Total loss: 3.5383 MSE Loss: 2.6295 MAE Loss: 0.9088 Val loss: 3.1975\n",
      "====> Epoch: 140, Train time: 578.4460, Val time: 0.4246\n",
      "====> Epoch: 140 Total loss: 3.5338 MSE Loss: 2.6264 MAE Loss: 0.9074 Val loss: 3.3324\n",
      "====> Epoch: 150, Train time: 618.9593, Val time: 0.3962\n",
      "====> Epoch: 150 Total loss: 3.5353 MSE Loss: 2.6266 MAE Loss: 0.9087 Val loss: 2.9485\n",
      "====> Epoch: 160, Train time: 657.2752, Val time: 0.4024\n",
      "====> Epoch: 160 Total loss: 3.5313 MSE Loss: 2.6242 MAE Loss: 0.9071 Val loss: 3.2919\n",
      "====> Epoch: 170, Train time: 696.1825, Val time: 0.4324\n",
      "====> Epoch: 170 Total loss: 3.5500 MSE Loss: 2.6393 MAE Loss: 0.9107 Val loss: 3.0636\n",
      "====> Epoch: 180, Train time: 736.9557, Val time: 0.4256\n",
      "====> Epoch: 180 Total loss: 3.5430 MSE Loss: 2.6343 MAE Loss: 0.9087 Val loss: 3.0191\n",
      "====> Epoch: 190, Train time: 777.1437, Val time: 0.5150\n",
      "====> Epoch: 190 Total loss: 3.5217 MSE Loss: 2.6144 MAE Loss: 0.9073 Val loss: 3.2667\n",
      "====Training complete====\n"
     ]
    }
   ],
   "source": [
    "if run_training:\n",
    "\n",
    "    enc, trainloss,valloss = train_model(\n",
    "        dl_model= deep_cal,\n",
    "        epochs = epochs, \n",
    "        training_dl = train_loader, \n",
    "        validation_dl = val_loader,\n",
    "        autograd = optim.AdamW, \n",
    "        #enc_loc = 'outputs/latent_space_verification/',\n",
    "        #dec_loc = 'outputs/latent_space_verification/' , \n",
    "        dlmodel_loc = 'equity/outputs/final/',\n",
    "        trainset_size = train_size,\n",
    "        valset_size = val_size, \n",
    "        trainloss_loc = 'equity/outputs/final/train_loss_', \n",
    "        valloss_loc = 'equity/outputs/final/val_loss_' , \n",
    "        learning_rate= 0.01, mse_multp = 1.0, mae_multp = 0.0, return_model = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run DL-model with live prices and compare with numerical calibration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoenc1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
